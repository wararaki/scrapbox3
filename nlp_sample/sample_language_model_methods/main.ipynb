{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0e47664dab37355857d58ec29513492f53a294cb5e9a32075fd110bf6e862dd2a",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Tuple\n",
    "\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "train_filename = \"wiki-en-train.word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(train_filename, \"rt\") as f:\n",
    "    for line in f:\n",
    "        words = [\"__BOS__\"] + line.lower().split() + [\"__EOS__\"]\n",
    "        sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.models import LanguageModel\n",
    "\n",
    "def show(model: LanguageModel, vocab: Vocabulary, context: Tuple[str], top_n: int=5):\n",
    "    scores = []\n",
    "    for word in model.context_counts(model.vocab.lookup(context)):\n",
    "        scores.append([word, model.score(word, context)])\n",
    "    scores = sorted(scores, key=lambda x: x[1])[::-1]\n",
    "\n",
    "    print(f\"Model: {model.__class__}\")\n",
    "    print(f\"The number of candidates: {len(scores)}\")\n",
    "    print(f\"Show top {top_n} probabilities:\")\n",
    "    for score in scores[:top_n]:\n",
    "        print(f\"P({score[0]}|{','.join(context)})={score[1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary(itertools.chain.from_iterable(sentences))\n",
    "context = (\"natural\", \"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE\n",
    "from nltk.lm.models import MLE\n",
    "mle = MLE(order=N, vocabulary=vocabulary)\n",
    "text_ngram = [ngrams(sentence, N) for sentence in sentences]\n",
    "mle.fit(text_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: <class 'nltk.lm.models.MLE'>\nThe number of candidates: 23\nShow top 5 probabilities:\nP(processing|natural,language)=0.4057971014492754\nP(understanding|natural,language)=0.17391304347826086\nP(generation|natural,language)=0.07246376811594203\nP(input|natural,language)=0.057971014492753624\nP(parsing|natural,language)=0.028985507246376812\n\n"
     ]
    }
   ],
   "source": [
    "show(mle, vocabulary, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Witten Bell Smoothing\n",
    "from nltk.lm.models import WittenBellInterpolated\n",
    "wlm = WittenBellInterpolated(order=N, vocabulary=vocabulary)\n",
    "text_ngram = [ngrams(sentence, N) for sentence in sentences]\n",
    "wlm.fit(text_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: <class 'nltk.lm.models.WittenBellInterpolated'>\nThe number of candidates: 23\nShow top 5 probabilities:\nP(processing|natural,language)=0.4055270709744075\nP(understanding|natural,language)=0.17379731613188892\nP(generation|natural,language)=0.07241554838828705\nP(input|natural,language)=0.05793243871062964\nP(parsing|natural,language)=0.02896621935531482\n\n"
     ]
    }
   ],
   "source": [
    "show(wlm, vocabulary, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneser-Ney smoothing\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "klm = KneserNeyInterpolated(order=N, vocabulary=vocabulary)\n",
    "text_ngram = [ngrams(sentence, N) for sentence in sentences]\n",
    "klm.fit(text_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: <class 'nltk.lm.models.KneserNeyInterpolated'>\nThe number of candidates: 23\nShow top 5 probabilities:\nP(processing|natural,language)=0.4043549137614905\nP(understanding|natural,language)=0.172470855790476\nP(generation|natural,language)=0.07102158042815718\nP(input|natural,language)=0.05652882680496877\nP(parsing|natural,language)=0.027543319558591958\n\n"
     ]
    }
   ],
   "source": [
    "show(klm, vocabulary, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}