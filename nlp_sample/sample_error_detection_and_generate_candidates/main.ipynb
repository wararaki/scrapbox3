{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0e47664dab37355857d58ec29513492f53a294cb5e9a32075fd110bf6e862dd2a",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import List\n",
    "\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "filepath = \"text/peachy/peachy-4304732.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(filepath, \"r\") as f:\n",
    "    for line in f.readlines()[2:]:\n",
    "        s = line.strip()\n",
    "        if len(s) == 0:\n",
    "            continue\n",
    "        sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "女性を潤す新たな注目ワードは“アミノ酸”\n8月も残すところ10日余り。秋のはじまりと共に気になってくるのがお肌の乾燥ケアだ。潤いとハリのある肌の為にヒアルロン酸とコラーゲンに着目した「ベキュア スキンパーフェクトスフィア」シリーズに、「アミノ酸」に着目したニューアイテムが登場した。\n皮膚成分のほとんどはアミノ酸によって占められており、素肌は良質なアミノ酸を常に求めている。今回新シリーズは、アルギニン、保湿作用のあるグルタミン酸、角質を形成するセリン、ハリ感をサポートするプロリンをカプセルに内包し、肌に速攻アプローチ。\n特に、ベキュア初となるバームタイプのメイク落としは、毛穴の奥の汚れまですっきり落としつつ、マッサージもできるという1品2役の楽ちんコスメ。マッサージだけだと続けるのは難しいが、毎日行うクレンジングと共に肌に溜まった汚れの排出を促し、すっきりとした輪郭とワントーン明るい肌を手に入れることができる。\nまた、十分な睡眠時間がとれず、肌本来の再生能力が不足している現代女性の為に睡眠中の肌潤い生理に注目したナイトクリームも登場。眠っている間に潤いとハリ感を与え、朝目覚めた時には艶めく素肌が手に入る。\n今年の秋の乾燥ケアにはアミノ酸。忙しい日々で失いがちな潤いを、手軽に補充し、艶肌を手に入れよう。アミノアクトクレンジングバーム、アミノアクトクレンジングフォーム、アミノエフェクティブスリープは8月20日より販売開始。\n・ベキュア オンラインショップ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary(itertools.chain.from_iterable(sentences))\n",
    "char_ngram = [ngrams(sentence, N) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('女', '性', 'を'), ('性', 'を', '潤'), ('を', '潤', 'す')]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "list(ngrams(sentences[0], N))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = MLE(order=N, vocabulary=vocabulary)\n",
    "lm.fit(char_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "を: 0.5\nの: 0.5\n"
     ]
    }
   ],
   "source": [
    "context = (\"女\", \"性\")\n",
    "for word in lm.context_counts(lm.vocab.lookup(context)):\n",
    "    print(f\"{word}: {lm.score(word, context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('ア', 'ミ', 'ノ'), ('ミ', 'ノ', 'ア'), ('ノ', 'ア', 'ミ')]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "text = \"アミノアミノクレンジング\"\n",
    "list(ngrams(text, N))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contexts = list(ngrams(text, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('ア', 'ミ', 'ノ'): 1.0\n('ミ', 'ノ', 'ア'): 0.25\n('ノ', 'ア', 'ミ'): 0.0\n('ア', 'ミ', 'ノ'): 1.0\n('ミ', 'ノ', 'ク'): 0.0\n('ノ', 'ク', 'レ'): 0\n('ク', 'レ', 'ン'): 1.0\n('レ', 'ン', 'ジ'): 1.0\n('ン', 'ジ', 'ン'): 1.0\n('ジ', 'ン', 'グ'): 1.0\n"
     ]
    }
   ],
   "source": [
    "scores = [0 for _ in range(len(text))]\n",
    "i = 0\n",
    "tp = 0\n",
    "for context in test_contexts:\n",
    "    p = lm.score(context[-1], context[:-1])\n",
    "    print(f\"{context}: {p}\")\n",
    "    if p <= tp:\n",
    "        for j in range(N):\n",
    "            scores[i+j] -= 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ア: 0 # ok\nミ: 0 # ok\nノ: -1 # ok\nア: -1 # ok\nミ: -2 # error!\nノ: -2 # error!\nク: -2 # error!\nレ: -1 # ok\nン: 0 # ok\nジ: 0 # ok\nン: 0 # ok\nグ: 0 # ok\n"
     ]
    }
   ],
   "source": [
    "ts = -2\n",
    "for c, score in zip(list(text), scores):\n",
    "    if score <= ts:\n",
    "        print(f\"{c}: {score} # error!\")\n",
    "    else:\n",
    "        print(f\"{c}: {score} # ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Pf(cm: List[str]) -> float:\n",
    "    pf = 1.0\n",
    "    for i in range(2, len(cm)):\n",
    "        pf *= lm.score(cm[i], (cm[i-2], cm[i-1]))\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Pb(cm: List[str]) -> float:\n",
    "    pb = 1.0\n",
    "    for i in range(2, len(cm)):\n",
    "        pb *= lm.score(cm[i-2], (cm[i-1], cm[i]))\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = dict() # {pos: [word]}\n",
    "ts = -2\n",
    "n_candidate_character = 300\n",
    "n_candidate_word = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ノア->ク: 1.0\n4 [['ク', 1.0]]\nアク->ト: 1.0\n4 [['ク', 'ト', 1.0]]\nアミ->ノ: 1.0\n5 [['ノ', 1.0]]\nミノ->酸: 0.625\nミノ->ア: 0.25\nミノ->エ: 0.125\n5 [['ノ', '酸', 0.625], ['ノ', 'ア', 0.25], ['ノ', 'エ', 0.125]]\nミノ->酸: 0.625\nミノ->ア: 0.25\nミノ->エ: 0.125\n6 [['酸', 0.625], ['ア', 0.25], ['エ', 0.125]]\nノ酸->”: 0.2\nノ酸->」: 0.2\nノ酸->に: 0.2\nノ酸->を: 0.2\nノ酸->。: 0.2\nノア->ク: 1.0\nノエ->フ: 1.0\n6 [['ア', 'ク', 0.25], ['エ', 'フ', 0.125], ['酸', '。', 0.125], ['酸', 'を', 0.125], ['酸', 'に', 0.125], ['酸', '」', 0.125], ['酸', '”', 0.125]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, len(scores)-2):\n",
    "    if scores[i] > ts:\n",
    "        continue\n",
    "\n",
    "    m1s = []\n",
    "    context = (text[i-2], text[i-1])\n",
    "    for m in lm.context_counts(lm.vocab.lookup(context)):\n",
    "        p = lm.score(m, (context))\n",
    "        print(f\"{''.join(context)}->{m}: {p}\")\n",
    "        if p <= 0:\n",
    "            continue\n",
    "        m1s.append([m, p])\n",
    "    m1s = sorted(m1s, key=lambda x: x[1])[::-1][:n_candidate_character]\n",
    "    print(i, m1s)\n",
    "\n",
    "    m2s = []\n",
    "    for m, p in m1s:\n",
    "        context = (text[i-1], m)\n",
    "        for m2 in lm.context_counts(lm.vocab.lookup(context)):\n",
    "            p2 = lm.score(m2, context)\n",
    "            print(f\"{''.join(context)}->{m2}: {p2}\")\n",
    "            if p2 <= 0:\n",
    "                continue\n",
    "            m2s.append([m, m2, p*p2])\n",
    "    m2s = sorted(m2s, key=lambda x: x[2])[::-1][:n_candidate_character]\n",
    "    print(i, m2s)\n",
    "\n",
    "    pfs = []\n",
    "    pbs = []\n",
    "    for m, _ in m1s:\n",
    "        pf = calc_Pf([text[i-2], text[i-1], m, text[i+1], text[i+2]])\n",
    "        pb = calc_Pb([text[i-2], text[i-1], m, text[i+1], text[i+2]])\n",
    "        pfs.append([m, pf])\n",
    "        pbs.append([m, pb])\n",
    "    pfs = sorted(pfs, key=lambda x: x[1])[::-1][:n_candidate_word]\n",
    "    pbs = sorted(pbs, key=lambda x: x[1])[::-1][:n_candidate_word]\n",
    "    can1 = list(set(m for m, _ in pfs+pbs))\n",
    "\n",
    "    pfs = []\n",
    "    pbs = []\n",
    "    for m, m2, _ in m2s:\n",
    "        pf = calc_Pf([text[i-2], text[i-1], m, m2, text[i+2], text[i+3]])\n",
    "        pb = calc_Pb([text[i-2], text[i-1], m, m2, text[i+2], text[i+3]])\n",
    "        pfs.append([m+m2, pf])\n",
    "        pbs.append([m+m2, pb])\n",
    "    pfs = sorted(pfs, key=lambda x: x[1])[::-1][:n_candidate_word]\n",
    "    pbs = sorted(pbs, key=lambda x: x[1])[::-1][:n_candidate_word]\n",
    "    can2 = list(set(w for w, _ in pfs+pbs))\n",
    "\n",
    "    candidates[i] = can1 + can2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'アミノアミノクレンジング'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'アミノアクトクレンジング'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "\"アミノアクトクレンジング\" # true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{4: ['ク', 'クト'],\n",
       " 5: ['ノ', 'ノエ', 'ノ酸', 'ノア'],\n",
       " 6: ['ア', '酸', 'エ', '酸」', '酸を', '酸。', '酸に', '酸”']}"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}